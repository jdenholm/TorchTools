

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torch_tools.models._variational_autoencoder_2d &mdash; TorchTools 0.13.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=92fd9be5" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=30f05f40" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=bbec6902"></script>
      <script src="../../../_static/documentation_options.js?v=dc0df7c6"></script>
      <script src="../../../_static/doctools.js?v=92e14aea"></script>
      <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            TorchTools
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html">FCNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._conv_net_2d">ConvNet2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._unet">UNet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._encoder_2d">Encoder2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._decoder_2d">Decoder2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._autoencoder_2d">AutoEncoder2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._variational_autoencoder_2d">VAE2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../models.html#module-torch_tools.models._simple_conv_2d">SimpleConvNet2d</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html">DataSet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html#module-torch_tools.datasets._shapes_dataset">ShapesDataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../misc.html">Misc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../torch_utils.html">Torch utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../weight_init.html">Weight initialisation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../file_utils.html">File utilities</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">TorchTools</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">torch_tools.models._variational_autoencoder_2d</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for torch_tools.models._variational_autoencoder_2d</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;2D convolutional variational autoencoder.&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>  <span class="c1"># pylint: disable=no-name-in-module</span>
    <span class="n">Tensor</span><span class="p">,</span>
    <span class="n">flatten</span><span class="p">,</span>
    <span class="n">unflatten</span><span class="p">,</span>
    <span class="n">randn_like</span><span class="p">,</span>
    <span class="n">set_grad_enabled</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span><span class="p">,</span> <span class="n">Sequential</span><span class="p">,</span> <span class="n">Conv2d</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch_tools.models._encoder_2d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Encoder2d</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_tools.models._decoder_2d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Decoder2d</span>

<span class="c1"># from torch_tools.models._decoder_2d import Decoder2d</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_tools.models._fc_net</span><span class="w"> </span><span class="kn">import</span> <span class="n">FCNet</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch_tools.models._argument_processing</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">process_num_feats</span><span class="p">,</span>
    <span class="n">process_u_architecture_layers</span><span class="p">,</span>
    <span class="n">process_str_arg</span><span class="p">,</span>
    <span class="n">process_negative_slope_arg</span><span class="p">,</span>
    <span class="n">process_2d_kernel_size</span><span class="p">,</span>
    <span class="n">process_input_dims</span><span class="p">,</span>
    <span class="n">process_optional_feats_arg</span><span class="p">,</span>
    <span class="n">process_dropout_prob</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch_tools.models._blocks_2d</span><span class="w"> </span><span class="kn">import</span> <span class="n">DoubleConvBlock</span>


<div class="viewcode-block" id="VAE2d">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">VAE2d</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>  <span class="c1"># pylint: disable=too-many-instance-attributes</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;2D convolutional variational autoencoder.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    in_chans : int</span>
<span class="sd">        The number of input channels the model should take.</span>
<span class="sd">    out_chans : int</span>
<span class="sd">        The number of output channels the model should produce.</span>
<span class="sd">    input_dims : Tuple[int, int]</span>
<span class="sd">        The ``(height, width)`` of the input images (only necessary if</span>
<span class="sd">        ``mean_var_nets == &quot;linear&quot;``).</span>
<span class="sd">    start_features : int, optional</span>
<span class="sd">        The number of features the first double conv block should produce.</span>
<span class="sd">    num_layers : int, optional</span>
<span class="sd">        The number of layers in the U-like architecture.</span>
<span class="sd">    down_pool : str, optional</span>
<span class="sd">        The type of pooling to use in the down-sampling layers: ``&quot;avg&quot;`` or</span>
<span class="sd">        ``&quot;max&quot;``.</span>
<span class="sd">    bilinear : bool, optional</span>
<span class="sd">        If ``True``, we use bilinear interpolation in the upsampling. If</span>
<span class="sd">        ``False``, we use ``ConvTranspose2d``.</span>
<span class="sd">    lr_slope : float, optional</span>
<span class="sd">        Negative slope to use in the leaky relu layers.</span>
<span class="sd">    kernel_size : int, optional</span>
<span class="sd">        Linear size of the square convolutional kernels to use.</span>
<span class="sd">    max_down_feats : int, optional</span>
<span class="sd">        Upper limit on the number of features that can be produced by the</span>
<span class="sd">        down-sampling blocks.</span>
<span class="sd">    min_up_feats : int, optional</span>
<span class="sd">        Minimum number of features the up-sampling blocks can produce.</span>
<span class="sd">    block_style : str</span>
<span class="sd">        Block style to use in the down and up blocks.</span>
<span class="sd">    mean_var_net : str</span>
<span class="sd">        The style of the networks for which learn the mean and variances:</span>
<span class="sd">        ``&quot;linear&quot;`` or ``&quot;conv&quot;``.</span>
<span class="sd">    dropout : float, optional</span>
<span class="sd">        Dropout probability to apply at the output of the convolutional blocks.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># pylint: disable=too-many-arguments,too-many-positional-arguments</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_chans</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_chans</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">input_dims</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">start_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">down_pool</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
        <span class="n">bilinear</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">lr_slope</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">max_down_feats</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">min_up_feats</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">block_style</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;double_conv&quot;</span><span class="p">,</span>
        <span class="n">mean_var_nets</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Build ``VAE2d``.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder2d</span><span class="p">(</span>
            <span class="n">in_chans</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="n">in_chans</span><span class="p">),</span>
            <span class="n">start_features</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="n">start_features</span><span class="p">),</span>
            <span class="n">num_blocks</span><span class="o">=</span><span class="n">process_u_architecture_layers</span><span class="p">(</span><span class="n">num_layers</span><span class="p">),</span>
            <span class="n">pool_style</span><span class="o">=</span><span class="n">process_str_arg</span><span class="p">(</span><span class="n">down_pool</span><span class="p">),</span>
            <span class="n">lr_slope</span><span class="o">=</span><span class="n">process_negative_slope_arg</span><span class="p">(</span><span class="n">lr_slope</span><span class="p">),</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">process_2d_kernel_size</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
            <span class="n">max_feats</span><span class="o">=</span><span class="n">process_optional_feats_arg</span><span class="p">(</span><span class="n">max_down_feats</span><span class="p">),</span>
            <span class="n">block_style</span><span class="o">=</span><span class="n">block_style</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">process_dropout_prob</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_latent_chans</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_latent_feats</span> <span class="o">=</span> <span class="n">_latent_sizes</span><span class="p">(</span>
            <span class="n">start_features</span><span class="p">,</span>
            <span class="n">num_layers</span><span class="p">,</span>
            <span class="n">process_input_dims</span><span class="p">(</span><span class="n">input_dims</span><span class="p">),</span>
            <span class="n">max_down_feats</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_input_dim_mean_var_net_check</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="n">mean_var_nets</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_style</span> <span class="o">=</span> <span class="n">mean_var_nets</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_funcs</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;linear&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_logvar_linear</span><span class="p">,</span>
            <span class="s2">&quot;conv&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_logvar_conv</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mean_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_or_var_net</span><span class="p">(</span><span class="n">lr_slope</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">var_net</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_or_var_net</span><span class="p">(</span><span class="n">lr_slope</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder2d</span><span class="p">(</span>
            <span class="n">in_chans</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_chans</span><span class="p">,</span>
            <span class="n">out_chans</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="n">out_chans</span><span class="p">),</span>
            <span class="n">num_blocks</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
            <span class="n">bilinear</span><span class="o">=</span><span class="n">bilinear</span><span class="p">,</span>
            <span class="n">lr_slope</span><span class="o">=</span><span class="n">lr_slope</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">min_up_feats</span><span class="o">=</span><span class="n">min_up_feats</span><span class="p">,</span>
            <span class="n">block_style</span><span class="o">=</span><span class="n">block_style</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">process_dropout_prob</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_input_dim_mean_var_net_check</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dims</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
        <span class="n">mean_var_nets</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check ``input_dims`` and ``mean_var_nets`` compatibility.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dims : Tuple[int, int] or None</span>
<span class="sd">            The size of the input image.</span>
<span class="sd">        mean_var_nets : str</span>
<span class="sd">            The style of the mean/variance nets.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;``input_dims`` should be ``None`` if ``mean_var_nets`` is &quot;</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot;``&#39;&#39;conv&#39;`` and ``Tuple[int, int]`` if ``mean_var_nets`` is &quot;</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;``&#39;linear&#39;``. Got &#39;</span><span class="si">{</span><span class="n">mean_var_nets</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">input_dims</span><span class="si">}</span><span class="s2">&#39;.&quot;</span>

        <span class="k">if</span> <span class="n">mean_var_nets</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mean_var_nets</span> <span class="o">==</span> <span class="s2">&quot;conv&quot;</span> <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_dims</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="kc">None</span><span class="p">))):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mean_or_var_net</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lr_slope</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Module</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a model for calculating the mean or variance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        lr_slope : float</span>
<span class="sd">            The negative slope aregument in the leaky relus.</span>
<span class="sd">        kernel_size : int</span>
<span class="sd">            The size of the kernel in the convolutional layers,</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Module</span>
<span class="sd">            A network for learning the mean or standard deviation.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        ValueError</span>
<span class="sd">            If ``mean_var_style`` is not ``&quot;conv&quot;`` or ``&quot;linear&quot;``.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_style</span> <span class="o">==</span> <span class="s2">&quot;conv&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Sequential</span><span class="p">(</span>
                <span class="n">DoubleConvBlock</span><span class="p">(</span>
                    <span class="n">in_chans</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_chans</span><span class="p">),</span>
                    <span class="n">out_chans</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_chans</span><span class="p">),</span>
                    <span class="n">lr_slope</span><span class="o">=</span><span class="n">process_negative_slope_arg</span><span class="p">(</span><span class="n">lr_slope</span><span class="p">),</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="n">process_2d_kernel_size</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                <span class="p">),</span>
                <span class="n">Conv2d</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_chans</span><span class="p">),</span>
                    <span class="n">out_channels</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_chans</span><span class="p">),</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_style</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">FCNet</span><span class="p">(</span>
                <span class="n">in_feats</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_feats</span><span class="p">),</span>  <span class="c1"># type: ignore</span>
                <span class="n">out_feats</span><span class="o">=</span><span class="n">process_num_feats</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_latent_feats</span><span class="p">),</span>  <span class="c1"># type: ignore</span>
            <span class="p">)</span>

        <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;mean_var_style &#39;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_style</span><span class="si">}</span><span class="s2">&#39; not recognised. Choose&quot;</span>
        <span class="n">msg</span> <span class="o">+=</span> <span class="s2">&quot; from &#39;conv&#39; or &#39;linear&#39;.&quot;</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mean_logvar_conv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Estimate the mean and logvar vector.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        features : Tensor</span>
<span class="sd">            Raw features from the encoders.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mean : Tensor</span>
<span class="sd">            The means.</span>
<span class="sd">        logvar : Tensor</span>
<span class="sd">            The logarithm of the variance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_mean_logvar_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Estimate the mean and logvar vector.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        features : Tensor</span>
<span class="sd">            Raw features from the encoders.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        mean : Tensor</span>
<span class="sd">            The means.</span>
<span class="sd">        logvar : Tensor</span>
<span class="sd">            The logarithm of the variance.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">restore_shape</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">flatten</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_net</span><span class="p">(</span><span class="n">features</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_net</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

        <span class="c1"># Restore shapes</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">restore_shape</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">restore_shape</span><span class="p">)</span>
        <span class="n">logvar</span> <span class="o">=</span> <span class="n">unflatten</span><span class="p">(</span><span class="n">logvar</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">restore_shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">logvar</span>

<div class="viewcode-block" id="VAE2d.get_features">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d.get_features">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">means</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">logvar</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the features using the reparam trick.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        means : Tensor</span>
<span class="sd">            The feature means.</span>
<span class="sd">        logvar : Tensor</span>
<span class="sd">            The log variance.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            The feature dist.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">means</span> <span class="o">+</span> <span class="p">(</span><span class="n">randn_like</span><span class="p">(</span><span class="n">means</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">())</span></div>


<div class="viewcode-block" id="VAE2d.kl_divergence">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d.kl_divergence">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">kl_divergence</span><span class="p">(</span><span class="n">means</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">log_var</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the KL divergence between the dists and a unit normal.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        means : Tensor</span>
<span class="sd">            Samples from the mean distributions.</span>
<span class="sd">        log_var : Tensor</span>
<span class="sd">            The logarithm of the variances.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            Kullback-Leibler divergence between the feature dists and unit</span>
<span class="sd">            normals.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">log_var</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span> <span class="o">-</span> <span class="n">means</span><span class="o">**</span><span class="mf">2.0</span> <span class="o">+</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="p">(</span><span class="n">log_var</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></div>


<div class="viewcode-block" id="VAE2d.encode">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d.encode">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">encode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">frozen_encoder</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Encode the inputs in ``batch``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch : Tensor</span>
<span class="sd">            Mini-batch of inputs.</span>
<span class="sd">        frozen_encoder : bool</span>
<span class="sd">            Shoould the encoder&#39;s weights be frozen, or not?</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        feats : Tensor</span>
<span class="sd">            The encoded features.</span>
<span class="sd">        Tensor</span>
<span class="sd">            The KL divergence between the features and N(0, 1).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">set_grad_enabled</span><span class="p">(</span><span class="ow">not</span> <span class="n">frozen_encoder</span><span class="p">):</span>
            <span class="n">encoder_feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="n">mean</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_funcs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_style</span><span class="p">](</span><span class="n">encoder_feats</span><span class="p">)</span>

            <span class="n">feats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_features</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">feats</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kl_divergence</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span></div>


<div class="viewcode-block" id="VAE2d.decode">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d.decode">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">frozen_decoder</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Decode the latent ``features``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        features : Tensor</span>
<span class="sd">            VA-encoded features.</span>
<span class="sd">        frozen_decoder : bool</span>
<span class="sd">            Should the decoder&#39;s weights be frozen, or not?</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            The decoded ``features``.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">set_grad_enabled</span><span class="p">(</span><span class="ow">not</span> <span class="n">frozen_decoder</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">features</span><span class="p">)</span></div>


<div class="viewcode-block" id="VAE2d.deterministic_pred">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d.deterministic_pred">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">deterministic_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Make a deterministic prediction from ``batch``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch : Tensor</span>
<span class="sd">            A mini-batch of inputs.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            An autoencoded version of ``batch``.</span>


<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        To make the prediction deterministic, we decode the predicted means.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">latent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">means</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_funcs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_mean_var_style</span><span class="p">](</span><span class="n">latent</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">means</span><span class="p">)</span></div>


<div class="viewcode-block" id="VAE2d.forward">
<a class="viewcode-back" href="../../../models.html#torch_tools.models._variational_autoencoder_2d.VAE2d.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">batch</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">frozen_encoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">frozen_decoder</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pass ``batch`` through the model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batch : Tensor</span>
<span class="sd">            A mini-batch of image-like inputs.</span>
<span class="sd">        frozen_encoder : bool, optional</span>
<span class="sd">            Should the encoder&#39;s parameters be fixed?</span>
<span class="sd">        frozen_decoder : bool, optional</span>
<span class="sd">            Should the decoder&#39;s weights be fixed?</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        decoded : Tensor</span>
<span class="sd">            The predicted version of ``batch``.</span>
<span class="sd">        kl_div : Tensor</span>
<span class="sd">            The KL divergence between ``features`` and N(0, 1).</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">features</span><span class="p">,</span> <span class="n">kl_div</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">frozen_encoder</span><span class="p">)</span>

        <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">frozen_decoder</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">decoded</span><span class="p">,</span> <span class="n">kl_div</span></div>
</div>



<span class="k">def</span><span class="w"> </span><span class="nf">_latent_sizes</span><span class="p">(</span>
    <span class="n">start_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">input_dims</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">max_feats</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the size of the features produced by the encoder.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    start_features : int</span>
<span class="sd">        The number features produced by the first block in the encoder.</span>
<span class="sd">    num_blocks : int</span>
<span class="sd">        The number of blocks in one half of the U-like architecture.</span>
<span class="sd">    input_dims : Tuple[int, int] or None</span>
<span class="sd">        The spatial dimensions of the model&#39;s inputs.</span>
<span class="sd">    max_feats : int, optional</span>
<span class="sd">        The maximum number of features allowed.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    latent_chans : int</span>
<span class="sd">        The number of channels the image-like representation has after it is</span>
<span class="sd">        encoded.</span>
<span class="sd">    latent_feats : int</span>
<span class="sd">        The total number of features in the latent space after encoding. If</span>
<span class="sd">        the mean and var nets are linear, this is the number of channels.</span>

<span class="sd">    Raises</span>
<span class="sd">    ------</span>
<span class="sd">    ValueError</span>
<span class="sd">        If the number of features would be reduced to zero because</span>
<span class="sd">        ``input_dims`` is too small for the number of layers.</span>


<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">latent_chans</span> <span class="o">=</span> <span class="n">start_features</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">max_feats</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">latent_chans</span> <span class="o">*=</span> <span class="mi">2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">latent_chans</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">max_feats</span><span class="p">,</span> <span class="n">latent_chans</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">input_dims</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">latent_feats</span> <span class="o">=</span> <span class="n">latent_chans</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">in_height</span><span class="p">,</span> <span class="n">in_width</span> <span class="o">=</span> <span class="n">input_dims</span>
        <span class="n">factor</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="n">num_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">out_height</span> <span class="o">=</span> <span class="n">in_height</span> <span class="o">/</span> <span class="n">factor</span>
        <span class="n">out_width</span> <span class="o">=</span> <span class="n">in_width</span> <span class="o">/</span> <span class="n">factor</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">out_height</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">out_width</span> <span class="o">%</span> <span class="mi">1</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Image dims &#39;</span><span class="si">{</span><span class="p">(</span><span class="n">in_height</span><span class="p">,</span><span class="w"> </span><span class="n">in_width</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39; can&#39;t be halved </span><span class="si">{</span><span class="n">num_blocks</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> times.&quot;</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="n">latent_feats</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">out_height</span><span class="p">)</span> <span class="o">*</span> <span class="nb">int</span><span class="p">(</span><span class="n">out_width</span><span class="p">)</span> <span class="o">*</span> <span class="n">latent_chans</span>
        <span class="k">if</span> <span class="n">latent_feats</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">input_dims</span><span class="si">}</span><span class="s2"> too small for number of layers.&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">latent_chans</span><span class="p">,</span> <span class="n">latent_feats</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, J. Denholm.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>